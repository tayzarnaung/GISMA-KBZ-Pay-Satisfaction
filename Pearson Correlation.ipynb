{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae038e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf0ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# Use the file that contains the translation and the translated sentiment score\n",
    "file_path = \"KBZ_Pay_Translated_with_Sentiment.csv\" \n",
    "target_variable = 'Rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f1e5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: KBZ_Pay_Translated_with_Sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Load the Cleaned and Translated Data ---\n",
    "print(f\"Loading data from: {file_path}\")\n",
    "try:\n",
    "    # Assuming the file was correctly saved with UTF-8-SIG in the last step\n",
    "    df = pd.read_csv(file_path, encoding='utf-8-sig') \n",
    "except Exception as e:\n",
    "    print(f\"!!! ERROR: Failed to load the file. Check file name and path. Details: {e}\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79de4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Data Cleanup and Preparation\n",
    "df = df.dropna(subset=[target_variable, 'Sentiment_Score_Translated'])\n",
    "df[target_variable] = pd.to_numeric(df[target_variable], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf0714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering: Converting Date to continuous 'Days_Since_Start'...\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Feature Engineering for Time (Date -> Days_Since_Start) ---\n",
    "print(\"Feature Engineering: Converting Date to continuous 'Days_Since_Start'...\")\n",
    "try:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    # Calculate a continuous variable: Days Since Start of the review period\n",
    "    df['Days_Since_Start'] = (df['Date'] - df['Date'].min()).dt.days\n",
    "except Exception as e:\n",
    "    print(f\"Error processing 'Date' column: {e}. Skipping this variable.\")\n",
    "    # Set to NaN so it doesn't break later steps if date parsing fails\n",
    "    df['Days_Since_Start'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0166d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering: Applying One-Hot Encoding to 'Device Type'...\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Feature Engineering for Device Type (One-Hot Encoding) ---\n",
    "print(\"Feature Engineering: Applying One-Hot Encoding to 'Device Type'...\")\n",
    "if 'Device Type' in df.columns:\n",
    "    # One-Hot Encoding (OHE) for the categorical variable 'Device Type'\n",
    "    device_dummies = pd.get_dummies(df['Device Type'], prefix='Device', drop_first=False)\n",
    "    df = pd.concat([df, device_dummies], axis=1)\n",
    "    \n",
    "    # Drop the original categorical column\n",
    "    df = df.drop(columns=['Device Type'])\n",
    "else:\n",
    "    print(\"Warning: 'Device Type' column not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f69a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Identify Final Variables for Correlation ---\n",
    "# Collect the names of all continuous and dummy independent variables\n",
    "\n",
    "independent_vars = [\n",
    "    'Sentiment_Score_Translated',\n",
    "    'Days_Since_Start',\n",
    "]\n",
    "\n",
    "# Add the 'Language' variable (Is_Burmese) if it exists from the translation step\n",
    "if 'Is_Burmese' in df.columns:\n",
    "    independent_vars.append('Is_Burmese')\n",
    "else:\n",
    "    # If translation was skipped, we check if the original Is_Burmese was present\n",
    "    print(\"Warning: 'Is_Burmese' column not found. Language variable omitted.\")\n",
    "\n",
    "# Dynamically add all OHE columns for Device Type\n",
    "independent_vars.extend([col for col in df.columns if col.startswith('Device_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af1fd054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "        FINAL PEARSON CORRELATION RESULTS (r)\n",
      "           Target Variable: Rating\n",
      "==================================================\n",
      "Sentiment_Score_Translated    0.4166\n",
      "Device_Tablet                 0.0032\n",
      "Device_Phone                 -0.0032\n",
      "Days_Since_Start             -0.0252\n",
      "Is_Burmese                   -0.2390\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Calculate the Pearson Correlation Matrix ---\n",
    "# Filter DataFrame to include only the target and independent variables for correlation\n",
    "correlation_data = df[[target_variable] + independent_vars].dropna()\n",
    "correlation_matrix = correlation_data.corr(method='pearson')\n",
    "\n",
    "# --- Step 6: Extract and Present Results ---\n",
    "# Extract the correlation coefficients between Rating and all independent variables\n",
    "pearson_results = correlation_matrix.loc[target_variable, independent_vars].sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"        FINAL PEARSON CORRELATION RESULTS (r)\")\n",
    "print(f\"           Target Variable: {target_variable}\")\n",
    "print(\"=\"*50)\n",
    "# Print results in a neat, structured format\n",
    "print(pearson_results.to_string(header=True, float_format=\"{:.4f}\".format))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b86d880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fully processed dataset saved to: KBZ_Pay_Fully_Processed_Data.csv\n"
     ]
    }
   ],
   "source": [
    "# Optional: Save the fully processed data for your records\n",
    "df.to_csv('KBZ_Pay_Fully_Processed_Data.csv', index=False, encoding='utf-8-sig')\n",
    "print(\"\\nFully processed dataset saved to: KBZ_Pay_Fully_Processed_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f82e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
